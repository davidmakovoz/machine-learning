{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de6a41f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "import os\n",
    "mistral_api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "model = \"mistral-tiny\"\n",
    "\n",
    "client = MistralClient(api_key=mistral_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01914997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mistralai, langchain_mistralai"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5cd82ac",
   "metadata": {},
   "source": [
    "mistral7b -> mistral-tiny\n",
    "mixtral8x7b -> mistral-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68fbe0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"What is the best French cheese?\")\n",
    "]\n",
    "model = 'mistral-small'\n",
    "# No streaming\n",
    "chat_response = client.chat(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "# # With streaming\n",
    "# for chunk in client.chat_stream(model=model, messages=messages):\n",
    "#     print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c64204c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'd99719396a7141f0a83013173b2ae487',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1708027822,\n",
       " 'model': 'mistral-small',\n",
       " 'choices': [ChatCompletionResponseChoice(index=0, message=ChatMessage(role='assistant', content='I don\\'t have personal experiences or opinions. However, it\\'s important to note that the \"best\" French cheese can vary greatly depending on personal preference, as there are over 400 types of cheese produced in France!\\n\\nSome people might prefer the strong and pungent flavor of a Roquefort, a blue cheese from the south of France. Others might enjoy the nutty and versatile Comté from the Jura mountains, or the soft and creamy Brie de Meaux. \\n\\nIf you\\'re a fan of goat\\'s cheese, you might love the tangy and bright flavor of a Picodon or a Selles-sur-Cher. If you prefer something milder, the buttery and subtle Saint-André or the semi-hard and slightly sweet Morbier could be to your liking.\\n\\nUltimately, the best French cheese is a matter of personal taste, so I would recommend trying several varieties to find the one that suits your palate the most!'), finish_reason=<FinishReason.stop: 'stop'>)],\n",
       " 'usage': UsageInfo(prompt_tokens=15, total_tokens=231, completion_tokens=216)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_response.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "282a4c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's subjective to determine the \"best\" French cheese as it largely depends on personal preferences. Here are a few popular and highly acclaimed French cheeses from different categories:\n",
      "\n",
      "1. Roquefort (Blue Cheese): Known for its distinctive pungent smell and tangy taste. Made from sheep's milk.\n",
      "2. Comté (Hard Cheese): A nutty and slightly sweet cheese with a rich, complex flavor. Made from cow's milk.\n",
      "3. Camembert de Normandie (Soft Cheese): A soft, creamy cheese with a pungent aroma and earthy, mushroomy taste. Made from cow's milk.\n",
      "4. Brie de Meaux (Soft Cheese): A soft, creamy cheese with a velvety texture and mild, buttery flavor. Made from cow's milk.\n",
      "5. Munster (Soft, Smear Ripened Cheese): A pungent, smear-ripened cheese with a strong, tangy flavor and distinctive orange rind. Made from cow's milk.\n",
      "6. Chavignol (Goat Cheese): A small, cylindrical goat cheese from the Loire Valley with a tangy, herbaceous flavor and a creamy texture.\n",
      "\n",
      "To enjoy the best French cheese, consider trying a variety to find your favorite. Cheese plates often include a mix of textures and milk sources for a well-rounded tasting experience. Enjoy your cheese with a good wine, a crusty baguette, or fresh fruit.\n"
     ]
    }
   ],
   "source": [
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d609c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have personal experiences or opinions. However, I can share that France is known for its diverse and high-quality cheeses, making it difficult to declare a single \"best\" one. The title of \"best French cheese\" can vary greatly depending on personal taste.\n",
      "\n",
      "Some popular and highly regarded French cheeses include:\n",
      "\n",
      "1. Brie de Meaux: A soft, creamy, and mild cheese often referred to as the \"queen of cheeses.\"\n",
      "2. Comté: A hard cheese with a nutty, sweet, and complex flavor. It's made in the Franche-Comté region.\n",
      "3. Roquefort: A blue cheese from the Roquefort-sur-Soulzon region, known for its tangy, sharp, and salty flavor.\n",
      "4. Camembert de Normandie: Another soft cheese, similar to Brie but with a stronger flavor and aroma.\n",
      "5. Reblochon: A soft, washed-rind cheese with a slightly fruity and earthy flavor, hailing from the Savoie region.\n",
      "\n",
      "Ultimately, the best French cheese depends on your individual taste preferences. I recommend trying various types to discover your favorite.\n"
     ]
    }
   ],
   "source": [
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3aec5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain.schema.messages import HumanMessage\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings, HuggingFaceBgeEmbeddings, OpenAIEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain, StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5056f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatMistralAI(mistral_api_key=mistral_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ce48758",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unsupported message type: <class 'langchain.schema.messages.HumanMessage'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msay a brief hello\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m----> 2\u001b[0m chat\u001b[38;5;241m.\u001b[39minvoke(messages)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\language_models\\chat_models.py:167\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    163\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    165\u001b[0m         ChatGeneration,\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m--> 167\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    168\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    169\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    170\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    171\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    172\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    173\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    174\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    175\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\language_models\\chat_models.py:148\u001b[0m, in \u001b[0;36mBaseChatModel._convert_input\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StringPromptValue(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, Sequence):\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39mconvert_to_messages(\u001b[38;5;28minput\u001b[39m))\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\messages\\__init__.py:231\u001b[0m, in \u001b[0;36mconvert_to_messages\u001b[1;34m(messages)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_messages\u001b[39m(\n\u001b[0;32m    221\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[0;32m    222\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseMessage]:\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a sequence of messages to a list of messages.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m        List of messages (BaseMessages).\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_convert_to_message(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\messages\\__init__.py:231\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_messages\u001b[39m(\n\u001b[0;32m    221\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[0;32m    222\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseMessage]:\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a sequence of messages to a list of messages.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m        List of messages (BaseMessages).\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_convert_to_message(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\messages\\__init__.py:215\u001b[0m, in \u001b[0;36m_convert_to_message\u001b[1;34m(message)\u001b[0m\n\u001b[0;32m    211\u001b[0m     _message \u001b[38;5;241m=\u001b[39m _create_message_from_message_type(\n\u001b[0;32m    212\u001b[0m         msg_type, msg_content, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmsg_kwargs\n\u001b[0;32m    213\u001b[0m     )\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported message type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(message)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _message\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Unsupported message type: <class 'langchain.schema.messages.HumanMessage'>"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"say a brief hello\")]\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79ec6cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='CRQ can have different meanings depending on the context, but in the fields of quality management and customer service, CRQ often stands for \"Critical-to-Quality.\"\\n\\nCritical-to-Quality (CTQ) is a term used to describe the specific product or process characteristics that have a significant impact on customer satisfaction. CTQs are the key features or aspects of a product or service that customers consider essential, and meeting or exceeding their expectations for these CTQs is crucial for delivering high-quality products or services.\\n\\nIdentifying and measuring CTQs is an important step in quality management and continuous improvement initiatives. By focusing on CTQs, organizations can prioritize their improvement efforts and resources on the areas that will have the greatest impact on customer satisfaction and loyalty.\\n\\nCTQs are typically identified through various methods, such as voice of the customer (VOC) surveys, customer feedback, and analysis of customer complaints. Once CTQs are identified, they can be used to develop specific quality metrics, targets, and improvement initiatives.\\n\\nOverall, CRQ or CTQ is a critical concept in quality management and customer service, as it helps organizations focus on the most important product or process characteristics that drive customer satisfaction and loyalty.')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What is CRQ?\")]\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2b0e660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ChatMistralAI in module langchain_mistralai.chat_models:\n",
      "\n",
      "class ChatMistralAI(langchain_core.language_models.chat_models.BaseChatModel)\n",
      " |  ChatMistralAI(*, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain_core.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: mistralai.client.MistralClient = None, async_client: mistralai.async_client.MistralAsyncClient = None, mistral_api_key: Optional[pydantic.v1.types.SecretStr] = None, endpoint: str = 'https://api.mistral.ai', max_retries: int = 5, timeout: int = 120, max_concurrent_requests: int = 64, model: str = 'mistral-small', temperature: float = 0.7, max_tokens: Optional[int] = None, top_p: float = 1, random_seed: Optional[int] = None, safe_mode: bool = False) -> None\n",
      " |  \n",
      " |  A chat model that uses the MistralAI API.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ChatMistralAI\n",
      " |      langchain_core.language_models.chat_models.BaseChatModel\n",
      " |      langchain_core.language_models.base.BaseLanguageModel\n",
      " |      langchain_core.runnables.base.RunnableSerializable\n",
      " |      langchain_core.load.serializable.Serializable\n",
      " |      pydantic.v1.main.BaseModel\n",
      " |      pydantic.v1.utils.Representation\n",
      " |      langchain_core.runnables.base.Runnable\n",
      " |      typing.Generic\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  completion_with_retry(self, run_manager: 'Optional[CallbackManagerForLLMRun]' = None, **kwargs: 'Any') -> 'Any'\n",
      " |      Use tenacity to retry the completion call.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  get_lc_namespace() -> 'List[str]' from pydantic.v1.main.ModelMetaclass\n",
      " |      Get the namespace of the langchain object.\n",
      " |  \n",
      " |  is_lc_serializable() -> 'bool' from pydantic.v1.main.ModelMetaclass\n",
      " |      Return whether this model can be serialized by Langchain.\n",
      " |  \n",
      " |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.v1.main.ModelMetaclass\n",
      " |      Validate api key, python package exists, temperature, and top_p.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  lc_secrets\n",
      " |      A map of constructor argument names to secret ids.\n",
      " |      \n",
      " |      For example,\n",
      " |          {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'async_client': 'MistralAsyncClient', 'client': 'Mi...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __config__ = <class 'pydantic.v1.config.Config'>\n",
      " |  \n",
      " |  __custom_root_type__ = False\n",
      " |  \n",
      " |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'me...\n",
      " |  \n",
      " |  __fields__ = {'async_client': ModelField(name='async_client', type=Opt...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __include_fields__ = None\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  __post_root_validators__ = [(False, <function BaseChatModel.raise_depr...\n",
      " |  \n",
      " |  __pre_root_validators__ = []\n",
      " |  \n",
      " |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      " |  \n",
      " |  __schema_cache__ = {}\n",
      " |  \n",
      " |  __signature__ = <Signature (*, cache: Optional[bool] = None, ver...al[...\n",
      " |  \n",
      " |  __validators__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |  \n",
      " |  __call__(self, messages: 'List[BaseMessage]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  async agenerate(self, messages: 'List[List[BaseMessage]]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Top Level call\n",
      " |  \n",
      " |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Asynchronously pass a sequence of prompts and return model generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      " |              converted to match the format of any language model (string for pure\n",
      " |              text generation models and BaseMessages for chat models).\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      Default implementation of ainvoke, calls invoke from a thread.\n",
      " |      \n",
      " |      The default implementation allows usage of async code even if\n",
      " |      the runnable did not implement a native async version of invoke.\n",
      " |      \n",
      " |      Subclasses should override this method if they can run asynchronously.\n",
      " |  \n",
      " |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      Asynchronously pass a string to the model and return a string prediction.\n",
      " |      \n",
      " |      Use this method when calling pure text generation models and only the top\n",
      " |          candidate generation is needed.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: String input to pass to the model.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Top model prediction as a string.\n",
      " |  \n",
      " |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      Asynchronously pass messages to the model and return a message prediction.\n",
      " |      \n",
      " |      Use this method when calling chat models and only the top\n",
      " |          candidate generation is needed.\n",
      " |      \n",
      " |      Args:\n",
      " |          messages: A sequence of chat messages corresponding to a single model input.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Top model prediction as a message.\n",
      " |  \n",
      " |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[BaseMessageChunk]'\n",
      " |      Default implementation of astream, which calls ainvoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |  \n",
      " |  call_as_llm(self, message: 'str', stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |  \n",
      " |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      " |      Return a dictionary of the LLM.\n",
      " |  \n",
      " |  generate(self, messages: 'List[List[BaseMessage]]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Top Level call\n",
      " |  \n",
      " |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Pass a sequence of prompts to the model and return model generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      " |              converted to match the format of any language model (string for pure\n",
      " |              text generation models and BaseMessages for chat models).\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      Transform a single input into an output. Override to implement.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: A config to use when invoking the runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The output of the runnable.\n",
      " |  \n",
      " |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      Pass a single string input to the model and return a string prediction.\n",
      " |      \n",
      " |       Use this method when passing in raw text. If you want to pass in specific\n",
      " |          types of chat messages, use predict_messages.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: String input to pass to the model.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Top model prediction as a string.\n",
      " |  \n",
      " |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      Pass a message sequence to the model and return a message prediction.\n",
      " |      \n",
      " |      Use this method when passing in chat messages. If you want to pass in raw text,\n",
      " |          use predict.\n",
      " |      \n",
      " |      Args:\n",
      " |          messages: A sequence of chat messages corresponding to a single model input.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Top model prediction as a message.\n",
      " |  \n",
      " |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[BaseMessageChunk]'\n",
      " |      Default implementation of stream, which calls invoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |  \n",
      " |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.v1.main.ModelMetaclass\n",
      " |      Raise deprecation warning if callback_manager is used.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |  \n",
      " |  OutputType\n",
      " |      Get the output type for this runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |  \n",
      " |  Config = <class 'langchain_core.language_models.chat_models.BaseChatMo...\n",
      " |      Configuration for this pydantic object.\n",
      " |  \n",
      " |  \n",
      " |  __orig_bases__ = (langchain_core.language_models.base.BaseLanguageMode...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |  \n",
      " |  get_num_tokens(self, text: 'str') -> 'int'\n",
      " |      Get the number of tokens present in the text.\n",
      " |      \n",
      " |      Useful for checking if an input will fit in a model's context window.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: The string input to tokenize.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The integer number of tokens in the text.\n",
      " |  \n",
      " |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      " |      Get the number of tokens in the messages.\n",
      " |      \n",
      " |      Useful for checking if an input will fit in a model's context window.\n",
      " |      \n",
      " |      Args:\n",
      " |          messages: The message inputs to tokenize.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The sum of the number of tokens across the messages.\n",
      " |  \n",
      " |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      " |      Return the ordered ids of the tokens in a text.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: The string input to tokenize.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      " |              in the text.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |  \n",
      " |  InputType\n",
      " |      Get the input type for this runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |  \n",
      " |  configurable_alternatives(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -> 'RunnableSerializable[Input, Output]'\n",
      " |  \n",
      " |  configurable_fields(self, **kwargs: 'AnyConfigurableField') -> 'RunnableSerializable[Input, Output]'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  __init__(self, **kwargs: Any) -> None\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      " |  \n",
      " |  __repr_args__(self) -> Any\n",
      " |      Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n",
      " |      \n",
      " |      Can either return:\n",
      " |      * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n",
      " |      * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n",
      " |  \n",
      " |  to_json(self) -> Union[langchain_core.load.serializable.SerializedConstructor, langchain_core.load.serializable.SerializedNotImplemented]\n",
      " |  \n",
      " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  lc_id() -> List[str] from pydantic.v1.main.ModelMetaclass\n",
      " |      A unique identifier for this class for serialization purposes.\n",
      " |      \n",
      " |      The unique identifier is a list of strings that describes the path\n",
      " |      to the object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  lc_attributes\n",
      " |      List of attribute names that should be included in the serialized kwargs.\n",
      " |      \n",
      " |      These attributes must be accepted by the constructor.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |  \n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |      \n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |  \n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> str\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |      \n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |  \n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |  \n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -> 'DictStrAny' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -> str from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |  \n",
      " |  validate(value: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.utils.Representation:\n",
      " |  \n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __repr_name__(self) -> str\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: str) -> str\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |  \n",
      " |  __str__(self) -> str\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |  \n",
      " |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSerializable[Other, Output]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |  \n",
      " |  async abatch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
      " |      Default implementation runs ainvoke in parallel using asyncio.gather.\n",
      " |      \n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |      \n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
      " |  \n",
      " |  async astream_log(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Optional[Any]') -> 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'\n",
      " |      Stream all output from a runnable, as reported to the callback system.\n",
      " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
      " |      \n",
      " |      Output is streamed as Log objects, which include a list of\n",
      " |      jsonpatch ops that describe how the state of the run has changed in each\n",
      " |      step, and the final state of the run.\n",
      " |      \n",
      " |      The jsonpatch ops can be applied in order to construct state.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: The config to use for the runnable.\n",
      " |          diff: Whether to yield diffs between each step, or the current state.\n",
      " |          with_streamed_output_list: Whether to yield the streamed_output list.\n",
      " |          include_names: Only include logs with these names.\n",
      " |          include_types: Only include logs with these types.\n",
      " |          include_tags: Only include logs with these tags.\n",
      " |          exclude_names: Exclude logs with these names.\n",
      " |          exclude_types: Exclude logs with these types.\n",
      " |          exclude_tags: Exclude logs with these tags.\n",
      " |  \n",
      " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of atransform, which buffers input and calls astream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |  \n",
      " |  batch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
      " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
      " |      \n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |      \n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
      " |  \n",
      " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind arguments to a Runnable, returning a new Runnable.\n",
      " |  \n",
      " |  config_schema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'Type[BaseModel]'\n",
      " |      The type of config this runnable accepts specified as a pydantic model.\n",
      " |      \n",
      " |      To mark a field as configurable, see the `configurable_fields`\n",
      " |      and `configurable_alternatives` methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate config.\n",
      " |  \n",
      " |  get_input_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate input to the runnable.\n",
      " |      \n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic input schema that depends on which\n",
      " |      configuration the runnable is invoked with.\n",
      " |      \n",
      " |      This method allows to get an input schema for a specific configuration.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate input.\n",
      " |  \n",
      " |  get_output_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate output to the runnable.\n",
      " |      \n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic output schema that depends on which\n",
      " |      configuration the runnable is invoked with.\n",
      " |      \n",
      " |      This method allows to get an output schema for a specific configuration.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate output.\n",
      " |  \n",
      " |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      " |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      " |      by calling invoke() with each input.\n",
      " |  \n",
      " |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of transform, which buffers input and then calls stream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |  \n",
      " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind config to a Runnable, returning a new Runnable.\n",
      " |  \n",
      " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,)) -> 'RunnableWithFallbacksT[Input, Output]'\n",
      " |      Add fallbacks to a runnable, returning a new Runnable.\n",
      " |      \n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original runnable fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that will try the original runnable, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |  \n",
      " |  with_listeners(self, *, on_start: 'Optional[Listener]' = None, on_end: 'Optional[Listener]' = None, on_error: 'Optional[Listener]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      on_start: Called before the runnable starts running, with the Run object.\n",
      " |      on_end: Called after the runnable finishes running, with the Run object.\n",
      " |      on_error: Called if the runnable throws an error, with the Run object.\n",
      " |      \n",
      " |      The Run object contains information about the run, including its id,\n",
      " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
      " |      added to the run.\n",
      " |  \n",
      " |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      " |      Create a new Runnable that retries the original runnable on exceptions.\n",
      " |      \n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait time\n",
      " |                                   between retries\n",
      " |          stop_after_attempt: The maximum number of attempts to make before giving up\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original runnable on exceptions.\n",
      " |  \n",
      " |  with_types(self, *, input_type: 'Optional[Type[Input]]' = None, output_type: 'Optional[Type[Output]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind input and output types to a Runnable, returning a new Runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  config_specs\n",
      " |      List configurable fields for this runnable.\n",
      " |  \n",
      " |  input_schema\n",
      " |      The type of input this runnable accepts specified as a pydantic model.\n",
      " |  \n",
      " |  output_schema\n",
      " |      The type of output this runnable produces specified as a pydantic model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from pydantic.v1.main.ModelMetaclass\n",
      " |      Parameterizes a generic class.\n",
      " |      \n",
      " |      At least, parameterizing a generic class is the *main* thing this method\n",
      " |      does. For example, for some generic class `Foo`, this is called when we\n",
      " |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |      \n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo(Generic[T]): ...`.\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from pydantic.v1.main.ModelMetaclass\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ChatMistralAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bab6b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatMistralAI(temperature=0.0, model='mistral-tiny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6804e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mistral-tiny'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "888e48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat(collection_name, use_json = False):\n",
    "    persist_directory = './chromadb/'\n",
    "    embed_path  = '/embeddings/'\n",
    "    \n",
    "    llm = ChatMistralAI(temperature=0.0)\n",
    "    embeddings = HuggingFaceBgeEmbeddings(model_name = 'BAAI/bge-large-en', cache_folder=embed_path)\n",
    "\n",
    "\n",
    "    client = chromadb.PersistentClient(path= persist_directory)\n",
    "    db = Chroma(\n",
    "        client=client,\n",
    "    collection_name = collection_name,\n",
    "        embedding_function=embeddings)\n",
    "    retriever = db.as_retriever(search_type = 'similarity', search_kwargs={'k':10})\n",
    "\n",
    "\n",
    "    combine_template = '''\n",
    "    Make your answer concise and precise.\n",
    "    Use the following pieces of context to answer the question at the end. \n",
    "    If the context does not have the answer, just say I don't know. If it does try to provide a detailed answer with at least 3 sentences. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer and don't return any sources.\n",
    "    If you do  know the answeer return  the answer and the references to the sources used by using the following notation: \n",
    "    If the answer was found in the first piece of context, return the answer and \"[1]\", \n",
    "    If the answer was found in the second piece of context, return the answer and \"[2]\", \n",
    "    If the answer was found in the first and second piece of context, return the answer and \"[1], [2]\"\n",
    "    \n",
    "    \n",
    "    \\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\n",
    "    '''\n",
    "\n",
    "    if use_json:\n",
    "        combine_template = 'You are assistant. Only reply with JSON. ' + combine_template\n",
    "        \n",
    "    combine_docs_chain = StuffDocumentsChain(\n",
    "     llm_chain=LLMChain( prompt=PromptTemplate(input_variables=['context', 'question'], template=combine_template), \n",
    "    llm=llm), document_variable_name='context')\n",
    "\n",
    "    template = '''\n",
    "    Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\n",
    "    in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'\n",
    "    '''\n",
    "\n",
    "    question_generator  = LLMChain( prompt=PromptTemplate(input_variables=['chat_history', 'question'], \n",
    "    template=template), llm=llm)\n",
    "\n",
    "    cqa5 = ConversationalRetrievalChain(retriever=retriever, return_source_documents=True,\n",
    "                           combine_docs_chain=combine_docs_chain, question_generator=question_generator)\n",
    "    \n",
    "    #print (cqa5)\n",
    "    return cqa5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6c05feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = './chromadb/'\n",
    "embed_path  = '/embeddings/'\n",
    "persist_directory = './chromadb/'\n",
    "collection_name = 'crowdstrike_cortex_1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edfff2cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cqa \u001b[38;5;241m=\u001b[39m create_chat(collection_name)\n",
      "Cell \u001b[1;32mIn[8], line 35\u001b[0m, in \u001b[0;36mcreate_chat\u001b[1;34m(collection_name, use_json)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_json:\n\u001b[0;32m     32\u001b[0m     combine_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou are assistant. Only reply with JSON. \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m combine_template\n\u001b[0;32m     34\u001b[0m combine_docs_chain \u001b[38;5;241m=\u001b[39m StuffDocumentsChain(\n\u001b[1;32m---> 35\u001b[0m  llm_chain\u001b[38;5;241m=\u001b[39mLLMChain( prompt\u001b[38;5;241m=\u001b[39mPromptTemplate(input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m], template\u001b[38;5;241m=\u001b[39mcombine_template), \n\u001b[0;32m     36\u001b[0m llm\u001b[38;5;241m=\u001b[39mllm), document_variable_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124min its original language.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mChat History:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{chat_history}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFollow Up Input: \u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStandalone question:\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m     43\u001b[0m question_generator  \u001b[38;5;241m=\u001b[39m LLMChain( prompt\u001b[38;5;241m=\u001b[39mPromptTemplate(input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m     44\u001b[0m template\u001b[38;5;241m=\u001b[39mtemplate), llm\u001b[38;5;241m=\u001b[39mllm)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\openai\\Lib\\site-packages\\langchain\\load\\serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)"
     ]
    }
   ],
   "source": [
    "cqa = create_chat(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9579c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "question = 'What is Crowdstrike?'\n",
    "response = cqa({'question':question, 'chat_history':chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aaac4801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CrowdStrike is a global cybersecurity leader that has redefined modern security with one of the world’s most advanced cloud-native platforms for protecting critical areas of enterprise risk, such as endpoints, cloud workloads, identity, and data [1], [2]. Powered by the CrowdStrike Security Cloud and world-class AI, the CrowdStrike Falcon® platform leverages real-time indicators of attack, threat intelligence, evolving adversary tradecraft, and enriched telemetry from across the enterprise to deliver hyper-accurate detections, automated protection and remediation, elite threat hunting, and prioritized observability of vulnerabilities [1], [2]. CrowdStrike Falcon deploys in minutes to deliver actionable intelligence and real-time protection from Day One, offering a single lightweight-agent architecture that enables customers to benefit from rapid and scalable deployment, superior protection and performance, reduced complexity, and immediate time-to-value [1], [2].'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5acce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

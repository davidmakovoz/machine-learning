{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7572c05a-0792-4d1c-ace2-c8ad7db26ab7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Securing AI/ ML Models With HiddenLayer's AISec Platform\n",
    "\n",
    "This tutorial describes how to use the HiddenLayer's AI Security Platform to detect and respond to an attack on a machine learning model. This guide will provide examples of using HiddenLayer's Model Scanner, Machine Learning Detection and Response, and Prompt-Analyzer. All detections made by the AISec Platform will map to one or more [Mitre Atlas](https://atlas.mitre.org/) tactics and techniques. All models and data in this demo are open source and available via [HuggingFace ðŸ¤—](https://huggingface.co).\n",
    "\n",
    "### Model Scanner ðŸŽ¯\n",
    "HiddenLayer's Model Scanner is a tool to statically scan and analyze models to identify any threats or vulnerabilities in the model artifact. It is always best practice to scan any code or artifact before it is loaded into memory. Lots of modern tooling can load a model from a 3rd party repository directly into memory and opens up a threat vector of downloading and loading a model with malicious intent.\n",
    "\n",
    "Supported Formats\n",
    "- Pickle (numpy, joblib, scikit-learn)\n",
    "- PyTorch (pickle, zip)\n",
    "- TensorFlow/ Keras (tf, h5, protobuf)\n",
    "- SafeTensors\n",
    "- Onnx\n",
    "\n",
    "##### Steps ðŸš€\n",
    "1. Setup environment for demo\n",
    "    1. Install libraries\n",
    "    2. Login and create API credentials for demo\n",
    "    3. Initalize AISec Platform client for demo\n",
    "2. Model Scanner Scenarios\n",
    "    1. Scan LLM \n",
    "        - Model - [Microsoft Phi-2](https://huggingface.co/microsoft/phi-2/tree/main)\n",
    "    2. Scan Production Ready Model \n",
    "        - Model - [openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32/tree/main)\n",
    "    3. Scan Non-Production Ready Model (FastAI 3rd Party Tool)\n",
    "        - Model - [fastai/fastbook_04_mnist_basics](https://huggingface.co/fastai/fastbook_04_mnist_basics/tree/main)\n",
    "    4. Scan Malicious Model \n",
    "        - Model - [ScanMe/Models](https://huggingface.co/ScanMe/Models/tree/main)\n",
    "\n",
    "\n",
    "\n",
    "##### â—ï¸ For any issues with the notebook or feedback on how we could improve the experience please contact support@hiddenlayer.com\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dd69e87-74c5-45cc-a4f9-145362f2b9fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setup environment for demo\n",
    "1. Install libraries\n",
    "2. Login and create API credentials for demo\n",
    "3. Initalize AISec Platform client for demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96df9a7c-52e8-4547-8ea7-51fb783a3da2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f39a5d58-65d2-4876-a5bb-8ff7d87561ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# install libraries needed for demo\n",
    "#%pip install httpx adversarial-robustness-toolbox scikit-learn numpy numba huggingface_hub requests torch Pillow transformers matplotlib datasets \"datasets[vision]\" nest-asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a38513a-4248-4612-bda7-6c79e9f37d11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Login and create API credentials for demo\n",
    "\n",
    "## â—ï¸ Credentials must be created and copied into the notebok to run demos\n",
    "\n",
    "Navigate to the [HiddenLayer Console](https://console.hiddenlayer.ai/admin?activeTab=apiKeys) and login to create new API credentials to use for this demo. Copy and paste the client id and client secret into the next step of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9f5c1b9-4755-45a0-86e4-797b3907e786",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# add client credentials for the demo\n",
    "import os\n",
    "client_id = os.getenv('HIDDENLAYER_CLIENT_ID')\n",
    "client_secret = os.getenv('HIDDENLAYER_CLIENT_SECRET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67ffeede-edd4-4e85-9170-57c44b86daec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Initalize AISec Platform client for demo\n",
    "\n",
    "The follow is the client code to drive this demo notebook. If there are any issues or bugs found please contact us at support@hiddenlayer.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c876ef75-b3c9-460e-a4c5-8ac4c5998989",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import httpx\n",
    "import huggingface_hub\n",
    "import re\n",
    "import os\n",
    "import base64\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from typing import Any, Callable, Optional\n",
    "from random import randint\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class AISecClient(object):\n",
    "    def __init__(self, client_id:str, client_secret: str, base_url: str = \"https://api.hiddenlayer.ai\", retry_count: int = 3, timeout: int = 30):\n",
    "        self.client_id = client_id\n",
    "        self.client_secret = client_secret\n",
    "        self.client_auth = httpx.BasicAuth(client_id, client_secret)\n",
    "        self.base_url = base_url\n",
    "        self.token_url = \"https://auth.hiddenlayer.ai/oauth2/token?grant_type=client_credentials\"\n",
    "        self.retry_count = retry_count\n",
    "        self.async_client = httpx.AsyncClient(timeout=None)\n",
    "\n",
    "    @staticmethod\n",
    "    def b64str(b: bytes) -> str:\n",
    "        \"\"\"\n",
    "        Convert bytes to base64 string\"\"\"\n",
    "        return base64.b64encode(b).decode()\n",
    "    \n",
    "    async def _set_token(self) -> str:\n",
    "        \"\"\"\n",
    "        Get access token from HiddenLayer API\"\"\"\n",
    "        err_exc = Exception(\"Error: Unable to retrieve token\")\n",
    "        resp = await self.async_client.post(self.token_url, auth=self.client_auth)\n",
    "        if resp.status_code != 200:\n",
    "            raise err_exc\n",
    "        content = resp.json()\n",
    "        if \"access_token\" not in content:\n",
    "            raise err_exc\n",
    "        self.token = content[\"access_token\"]\n",
    "        self.async_client.headers = {\"Authorization\": f\"Bearer {self.token}\"}\n",
    "\n",
    "    async def _async_request_handler(self, meth: Callable, url: str, **kwargs: Any) -> httpx.Response:\n",
    "        \"\"\"\n",
    "        Handle async requests to HiddenLayer API\"\"\"\n",
    "        resp = None\n",
    "        for i in range(self.retry_count + 1):\n",
    "            resp = await meth(url, **kwargs)\n",
    "            if resp.status_code == 401:\n",
    "                await self._set_token()\n",
    "                continue\n",
    "            elif resp.status_code < 500:\n",
    "                break\n",
    "            await asyncio.sleep(randint(1, i + 1) / 100)  # nosec\n",
    "        return resp\n",
    "    \n",
    "    async def create_sensor(self, name: str, tags: dict = None):\n",
    "        \"\"\"\n",
    "        Create a sensor in HiddenLayer\"\"\"\n",
    "        tags = {} if tags is None else tags\n",
    "        resp = await self._async_request_handler(\n",
    "            self.async_client.post, \n",
    "            f\"{self.base_url}/api/v2/sensors/create\", \n",
    "            json={\"plaintext_name\": name, \"active\": True, \"tags\": tags}\n",
    "        )\n",
    "        return resp.json() if resp.is_success else None\n",
    "    \n",
    "    async def get_sensor_by_name_version(self, name: str, version: int):\n",
    "        \"\"\"\n",
    "        Get a sensor by name and version\"\"\"\n",
    "        sensor = None\n",
    "        resp = await self._async_request_handler(\n",
    "            self.async_client.post, \n",
    "            f\"{self.base_url}/api/v2/sensors/query\", \n",
    "            json={\"filter\":{\"plaintext_name\": name, \"version\": version}}\n",
    "        )\n",
    "        if resp.is_success:\n",
    "            content = resp.json()\n",
    "            if content[\"total_count\"] >= 1:\n",
    "                sensor = content[\"results\"][0]\n",
    "        return sensor\n",
    "\n",
    "    async def get_or_create_sensor(self, name: str, version: int = 1, tags: dict = None):\n",
    "        \"\"\"\n",
    "        Get or create a sensor by name and version\"\"\"\n",
    "        sensor = None\n",
    "        tags = {} if tags is None else tags\n",
    "        sensor = await self.get_sensor_by_name_version(name, version)\n",
    "        if sensor is None:\n",
    "            sensor = await self.create_sensor(name, tags=tags)\n",
    "        return sensor\n",
    "\n",
    "    async def _start_multipart_upload(self, sensor_id: str, filesize: int) -> dict:\n",
    "        \"\"\"\n",
    "        Start a multipart upload for a sensor\"\"\"\n",
    "        headers = {\"X-Content-Length\": str(filesize)}\n",
    "        resp = await self._async_request_handler(\n",
    "            self.async_client.post,\n",
    "            f\"{self.base_url}/api/v2/sensors/{sensor_id}/upload/begin\",\n",
    "            headers=headers\n",
    "        )\n",
    "        return resp.json() if resp.is_success else None\n",
    "\n",
    "    async def _upload_parts(self, sensor_id: str, multipart: dict, filepath: str) -> None:\n",
    "        \"\"\"\n",
    "        Upload parts for a multipart upload\"\"\"\n",
    "        chunk = 4\n",
    "        upload_id = multipart[\"upload_id\"]\n",
    "        with open(filepath, \"rb\") as fin:\n",
    "            for i in range(0, len(multipart[\"parts\"]), chunk):\n",
    "                upload_tasks = []\n",
    "                group = multipart[\"parts\"][i:i+chunk]\n",
    "                for p in group:\n",
    "                    part_number = p[\"part_number\"]\n",
    "                    read_amt = p[\"end_offset\"] - p[\"start_offset\"]\n",
    "                    fin.seek(p[\"start_offset\"])\n",
    "                    part_data = fin.read(read_amt)\n",
    "                    t = self._async_request_handler(\n",
    "                        self.async_client.put, \n",
    "                        f\"{self.base_url}/api/v2/sensors/{sensor_id}/upload/{upload_id}/part/{part_number}\", data=part_data\n",
    "                    )\n",
    "                    upload_tasks.append(t)\n",
    "                results = await asyncio.gather(*upload_tasks)\n",
    "\n",
    "    async def _complete_multipart_upload(self, sensor_id: str, upload_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        Complete a multipart upload\"\"\"\n",
    "        resp = await self._async_request_handler(\n",
    "            self.async_client.post,\n",
    "            f\"{self.base_url}/api/v2/sensors/{sensor_id}/upload/{upload_id}/complete\",\n",
    "        )\n",
    "        return resp.is_success\n",
    "\n",
    "    async def upload(self, sensor_id: str, filepath: str, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Upload a model to HiddenLayer\"\"\"\n",
    "        filesize = os.path.getsize(filepath)\n",
    "        # start multipart upload\n",
    "        if verbose:\n",
    "            print(f\"Starting upload for {sensor_id}: {filepath}\")\n",
    "        multipart = await self._start_multipart_upload(sensor_id, filesize)\n",
    "        # upload parts\n",
    "        await self._upload_parts(sensor_id, multipart, filepath)\n",
    "        # complete multipart upload\n",
    "        success = await self._complete_multipart_upload(sensor_id, multipart[\"upload_id\"])\n",
    "        if verbose:\n",
    "            print(f\"Completed upload for {sensor_id}: {filepath}\")\n",
    "        return success\n",
    "\n",
    "    async def scan_sensor(self, sensor_id: str):\n",
    "        \"\"\"\n",
    "        Scan a sensor in HiddenLayer after upload\"\"\"\n",
    "        # kick off scan for sensor id\n",
    "        resp = await self._async_request_handler(\n",
    "            self.async_client.post,\n",
    "            f\"{self.base_url}/api/v2/submit/sensors/{sensor_id}/scan\"\n",
    "        )\n",
    "        return resp.is_success\n",
    "        \n",
    "    async def upload_and_scan(self, model_name: str, filepath: str, version: int = 1, tags: dict = None, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Upload and scan a model\"\"\"\n",
    "        tags = {} if tags is None else tags\n",
    "        sensor = await self.get_or_create_sensor(model_name, version=version, tags=tags)\n",
    "        sensor_id = sensor[\"sensor_id\"]\n",
    "        await self.upload(sensor_id, filepath, verbose=verbose)\n",
    "        ok = await self.scan_sensor(sensor_id)\n",
    "        return ok, sensor\n",
    "\n",
    "    async def get_scan_results(self, sensor_id: str, max_retry_count: int = 160, verbose: bool = False, wait: bool = True):\n",
    "        \"\"\"\n",
    "        Get scan results for a sensor\"\"\"\n",
    "        retry = 1\n",
    "        results = None\n",
    "        while retry < max_retry_count: \n",
    "            resp = await self._async_request_handler(\n",
    "                self.async_client.get,\n",
    "                f\"{self.base_url}/api/v2/scan/status/{sensor_id}\"\n",
    "            )\n",
    "            if not resp.is_success:\n",
    "                break\n",
    "            else:\n",
    "                content = resp.json()\n",
    "                status = content[\"status\"]\n",
    "                if not wait or status == \"done\":\n",
    "                    results = content\n",
    "                    break\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print(f\"status, {sensor_id}, {status}\")\n",
    "                    await asyncio.sleep(5)  \n",
    "            retry += 1\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def pretty_print_scan_results(self, scan_results: dict):\n",
    "        \"\"\"\n",
    "        Pretty print scan results\"\"\"\n",
    "        for sensor_id, scan_result in scan_results.items():\n",
    "            verdict = \"SAFE\" if len(scan_result[\"scan_results\"][\"detections\"]) == 0 else \"UNSAFE\"\n",
    "            print(f\"{scan_result['sensor']['plaintext_name']} ({sensor_id})\")\n",
    "            print(\"#\"*128)\n",
    "            print(f\"{'Verdict':<25}{verdict:<48}\")\n",
    "            try:\n",
    "                print(f\"{'File':<25}{scan_result['filename']:<48}\")\n",
    "                print(f\"{'Type':<25}{scan_result['scan_results']['results']['type']:<48}\")\n",
    "                print(f\"{'MD5':<25}{scan_result['scan_results']['results']['md5']:<48}\")\n",
    "                print(f\"{'SHA256':<25}{scan_result['scan_results']['results']['sha256']:<48}\")\n",
    "                print(f\"{'TLSH':<25}{scan_result['scan_results']['results']['tlsh']:<48}\")\n",
    "                if len(scan_result[\"scan_results\"][\"detections\"]) > 0:\n",
    "                    print(\"Detections: \")\n",
    "                    print(f\"{'Severity':<25}{'Detection':<48}{'Description':<64}\")\n",
    "                    print(\"-\"*128)\n",
    "                    for d in scan_result[\"scan_results\"][\"detections\"]:\n",
    "                        print(f\"{d['severity']:<25}{d['message']:<48}{d['description']:<64}\")\n",
    "            except Exception as err:\n",
    "                print(f\"Error printing scan results: {err}\")\n",
    "            print(\"\\n\")\n",
    "    \n",
    "    def find_model_files_in_hf_repo(self, repo: str):\n",
    "        \"\"\"\n",
    "        Find model files in a HuggingFace repo\"\"\"\n",
    "        # read .gitattributes\n",
    "        # create regex patterns\n",
    "        # match model files and mark others as skipped\n",
    "        try:\n",
    "            model_info = huggingface_hub.model_info(repo)\n",
    "        except Exception:\n",
    "            return {}\n",
    "\n",
    "        gitattr = hf_hub_download(repo_id=repo, filename=\".gitattributes\")\n",
    "        with open(gitattr, \"r\") as fin:\n",
    "            patterns = [\n",
    "                re.compile(line.split(\" \")[0].replace(\".\", \"\\.\").replace(\"*\", \".*\") + \"$\")\n",
    "                for line in fin\n",
    "                if line\n",
    "            ]                    \n",
    "\n",
    "        return {\n",
    "            s.rfilename: any(p.match(s.rfilename) for p in patterns)\n",
    "            for s in model_info.siblings\n",
    "        }\n",
    "\n",
    "    async def scan_huggingface_repo(self, repo: str, verbose: bool = False, filename: str = None, scan:bool = True):\n",
    "        \"\"\"\n",
    "        Scan a HuggingFace repo\"\"\"\n",
    "        scan_results = {}\n",
    "        if filename is None:\n",
    "            model_files = self.find_model_files_in_hf_repo(repo)\n",
    "        else:\n",
    "            model_files = {filename: True}\n",
    "\n",
    "        model_version = 1\n",
    "        for i, (f, is_model) in enumerate(model_files.items()):\n",
    "            if is_model:\n",
    "                if verbose:\n",
    "                    print(f\"Downloading {repo}/{f}\")\n",
    "                fn = hf_hub_download(repo_id=repo, filename=f)\n",
    "                if scan:\n",
    "                    ok, sensor = await self.upload_and_scan(repo, fn, version=model_version, verbose=verbose, tags={\"env\":\"demo\", \"source\": \"huggingface\"})\n",
    "                else:\n",
    "                    sensor = await self.get_or_create_sensor(repo, version=model_version, tags={\"env\":\"demo\", \"source\": \"huggingface\"})\n",
    "                sensor_scan_results = await self.get_scan_results(sensor[\"sensor_id\"], wait=scan)\n",
    "                scan_results[sensor[\"sensor_id\"]] = {\"sensor\": sensor, \"uploaded\": scan, \"scan_results\": sensor_scan_results, \"filename\": f}\n",
    "                model_version += 1\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"Skipping {repo}/{f}\")\n",
    "        return scan_results\n",
    "\n",
    "\n",
    "client = AISecClient(client_id, client_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e01ec07a-4ea8-4e8e-99a3-2fd3ee10c3d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "516aa8cb-5eaf-46f7-9a70-8ea6104affe7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Model Scanner Scenarios\n",
    "1. Scan LLM \n",
    "    - Model - [Microsoft Phi-2](https://huggingface.co/microsoft/phi-2/tree/main)\n",
    "2. Scan Production Ready Model \n",
    "    - Model - [openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32/tree/main)\n",
    "3. Scan Non-Production Ready Model (FastAI 3rd Party Tool)\n",
    "    - Model - [fastai/fastbook_04_mnist_basics](https://huggingface.co/fastai/fastbook_04_mnist_basics/tree/main)\n",
    "4. Scan Malicious Model \n",
    "    - Model - [ScanMe/Models](https://huggingface.co/ScanMe/Models/tree/main)\n",
    "\n",
    "### â—ï¸ Note: After the models have been uploaded to HiddenLayer, they will show up in the [Model Inventory](https://console.hiddenlayer.ai/model-inventory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d95bee2b-48b8-4a64-9ca4-f7bcd1bb2ffb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Scan LLM âœ…\n",
    "\n",
    "In this section we will scan the [Microsoft Phi-2](https://huggingface.co/microsoft/phi-2/tree/main) LLM. This section can take 10-15 minutes to run due to downloading, uploading, and scanning the model that is 5Gb in size.\n",
    "\n",
    "###### â—ï¸ Note: Since the Phi-2 model is multiple artifacts. Two artifacts will show in the model overview view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2adf13f2-4696-4da7-a409-02bbdd8921c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning the microsoft/phi-2 model repo...\n",
      "Model microsoft/phi-2 wil be available in model-inventory soon...\n",
      "The microsoft/phi-2 scan is complete...\n",
      "\n",
      "microsoft/phi-2 (e25fb73b-5974-4884-9350-080d3d53a590)\n",
      "################################################################################################################################\n",
      "Verdict                  SAFE                                            \n",
      "File                     model-00001-of-00002.safetensors                \n",
      "Type                     safetensors                                     \n",
      "MD5                      2568cd93356c436b9c828a240af37a27                \n",
      "SHA256                   7fbcdefa72edf7527bf5da40535b57d9f5bd3d16829b94a9d25d2b457df62e84\n",
      "TLSH                     ceba23e3b1e1b68f8015dc6e4b19fa3419ebcd275c43e590b188868fd83da615f58fa0\n",
      "\n",
      "\n",
      "microsoft/phi-2 (c7a9d9a8-0419-436b-8064-efa8ff46824d)\n",
      "################################################################################################################################\n",
      "Verdict                  SAFE                                            \n",
      "File                     model-00002-of-00002.safetensors                \n",
      "Type                     safetensors                                     \n",
      "MD5                      04dcdd8f28d25c50d29deeb711f0005f                \n",
      "SHA256                   17b98759e4b7953cbcf63ec49be7edbc9b863b57c207d84a52f5d2f5bcfcf6b4\n",
      "TLSH                     654923e3b0e1b68f8415c85e4b59fa3018dbcd375887e4a0b188868fd97de215f59fa0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "repo_name = \"microsoft/phi-2\"\n",
    "print(f\"Scanning the {repo_name} model repo...\")\n",
    "print(f\"Model {repo_name} wil be available in model-inventory soon...\")\n",
    "scan_results = await client.scan_huggingface_repo(repo_name)\n",
    "print(f\"The {repo_name} scan is complete...\\n\")\n",
    "client.pretty_print_scan_results(scan_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0551743f-07e5-44ff-b823-9d76816b2b1b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Scan Production Ready Model âœ…\n",
    "\n",
    "This section will scan a specifc model from a model repository. It will select the `pytorch_model.bin` file from the [OpenAI Clip-Vit-Base-Patch32](https://huggingface.co/openai/clip-vit-base-patch32/tree/main) model repo. \n",
    "\n",
    "Notes:\n",
    "- This model has the expected pickle modules for a model that is ready for production\n",
    "- Only modules need to reload the tensors are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7031736d-faa7-41bf-b227-b90d215ac3a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning the openai/clip-vit-base-patch32 model repo...\n",
      "Model openai/clip-vit-base-patch32 wil be available in model-inventory soon...\n",
      "The openai/clip-vit-base-patch32 scan is complete...\n",
      "\n",
      "openai/clip-vit-base-patch32 (1c565544-22da-45e2-b919-c3fe1b76c510)\n",
      "################################################################################################################################\n",
      "Verdict                  SAFE                                            \n",
      "File                     pytorch_model.bin                               \n",
      "Type                     pytorch                                         \n",
      "MD5                      47767ea81d24718fcc0c8923607792a7                \n",
      "SHA256                   a63082132ba4f97a80bea76823f544493bffa8082296d62d71581a4feff1576f\n",
      "TLSH                     7a597481e1068fd0bca17b7bb8bf5d4e8edbca14d1bb10509726517da35b1d02fa3268\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "repo_name = \"openai/clip-vit-base-patch32\"\n",
    "print(f\"Scanning the {repo_name} model repo...\")\n",
    "print(f\"Model {repo_name} wil be available in model-inventory soon...\")\n",
    "scan_results = await client.scan_huggingface_repo(repo_name, filename=\"pytorch_model.bin\")\n",
    "print(f\"The {repo_name} scan is complete...\\n\")\n",
    "client.pretty_print_scan_results(scan_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1910644e-24f5-48f6-8bba-8e6b90958d2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Scan Non-Production Ready Model âœ…\n",
    "\n",
    "This section will scan a specifc model from a model repository. It will select the `model.pkl` file from the [fastai/fastbook_04_mnist_basics](https://huggingface.co/fastai/fastbook_04_mnist_basics/tree/main) model repo. \n",
    "\n",
    "Notes:\n",
    "- This model has a lot of pickle imports, making it more risky to introduce into a production environment\n",
    "  - Some legitimate use cases such as debugging, remote telemetry, and monitoring\n",
    "- `global.__getattr__` being found can add additional risk due to being an usafe way to execute python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dcd7ace-ccd6-4163-b3c0-bcb6eb7d960e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning the fastai/fastbook_04_mnist_basics model repo...\n",
      "Model fastai/fastbook_04_mnist_basics wil be available in model-inventory soon...\n",
      "The fastai/fastbook_04_mnist_basics scan is complete...\n",
      "\n",
      "fastai/fastbook_04_mnist_basics (290cc15c-e514-4692-9d23-efc3e354ba86)\n",
      "################################################################################################################################\n",
      "Verdict                  UNSAFE                                          \n",
      "File                     model.pkl                                       \n",
      "Type                     pytorch                                         \n",
      "MD5                      87e0800ed11fffc1700b31803fbfb8cf                \n",
      "SHA256                   01aad6e71a0e92b73dfa1332688ca991f3b2270b8e81eefbacce648f6b320cc6\n",
      "TLSH                     7da733c1ab3f614ad83520ae836990c37b48e0ef6b3bd6d716e2fd492c750425ec56c6\n",
      "Detections: \n",
      "Severity                 Detection                                       Description                                                     \n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "SUSPICIOUS               pickle-str___builtin___getattr_global_inst      This detection rule was triggered by the presence of function or library that can be used to return a callable object. Offending module / function:builtin.getattr or operator.attrgetter.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "repo_name = \"fastai/fastbook_04_mnist_basics\"\n",
    "print(f\"Scanning the {repo_name} model repo...\")\n",
    "print(f\"Model {repo_name} wil be available in model-inventory soon...\")\n",
    "scan_results = await client.scan_huggingface_repo(repo_name, filename=\"model.pkl\")\n",
    "print(f\"The {repo_name} scan is complete...\\n\")\n",
    "client.pretty_print_scan_results(scan_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c71ec58e-d994-42d8-a5b0-494abaed9e69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Scan malicious model âŒ\n",
    "\n",
    "â—ï¸ Note: Be careful when handling unsafe models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc0ef10-0554-4c30-8b5d-332246ea647f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning the ScanMe/Models model repo...\n",
      "Model ScanMe/Models wil be available in model-inventory soon...\n",
      "The ScanMe/Models scan is complete...\n",
      "\n",
      "ScanMe/Models (9ad2ae99-809f-49cb-8b11-c29c70ec15ee)\n",
      "################################################################################################################################\n",
      "Verdict                  UNSAFE                                          \n",
      "File                     eval_lambda.h5                                  \n",
      "Type                     keras                                           \n",
      "MD5                      139152c3ae4ed27124d4217079e8b6e7                \n",
      "SHA256                   d9ceae4da8037b02280a23368325ddda263889ce11771c4a78301aac1b2254ba\n",
      "TLSH                     a012ca37ab21dd3fd0b99838048643b92b20df4317c15747a690b92c3eb58585f61cd9\n",
      "Detections: \n",
      "Severity                 Detection                                       Description                                                     \n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "MALICIOUS                keras-lambda-function                           Found lambda embedded in keras model allowing custom layers that support  arbitrary expression execution\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "repo_name = \"ScanMe/Models\"\n",
    "print(f\"Scanning the {repo_name} model repo...\")\n",
    "print(f\"Model {repo_name} wil be available in model-inventory soon...\")\n",
    "scan_results = await client.scan_huggingface_repo(repo_name, filename=\"eval_lambda.h5\")\n",
    "print(f\"The {repo_name} scan is complete...\\n\")\n",
    "client.pretty_print_scan_results(scan_results)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "HiddenLayer_AISec_Platform_Notebook_v2",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
